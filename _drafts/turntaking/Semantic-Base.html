---
layout: post
author: Erik
---


Turn-taking is at its base define by the semantic content of the interlocutors.


<!--more-->


<h1>Background</h1>


<h2>TCU & TRP </h2>

Sources:
<ul>
  <li> Wikipedia <a href="https://en.wikipedia.org/wiki/Turn_construction_unit" target="_blank">TCU</a> </li>
  <li> <a href="https://pure.mpg.de/rest/items/item_2376846/component/file_2376845/content" target="_blank">A simplest systematics for the organization of turn-taking for conversation </a> (Sacks, Schegloff & Jefferson 1974)
  </li>
</ul>


<p>
For the purpose of turn-taking the moments of interest in a conversation are those places where it is appropriate to take the turn and those where it is not. A conversation
transitions between these two states. Sacks et al defined a turn construction unit, TCU, which is a segment that could describe an entire turn or a subset. TCUs are
separated by turn relevance places, TRP,  which are a opportunities for the other speaker to take the turn. Lets create an example. 
</p>

<p>
<br>
A: What is your favorite ice cream flavor?

<br>
--Silence--

<br>
A: ...or which ones do you like the best?

<br>
--Silence--

<br>
B: I like chocoalate
</p>

<p>
In the smple conversation above we see two utterances by A, both questions. The turn of A ends after the second question but it would be reasonable that it would end after
the first question. In this case The two utterances by A are both TCUs, the silence after both the utterances are TRPs but only the latter one was realized.
</p>


There exists some classes of TCU which depend on the linguistic "size" of the utterance, that is lexical, phrasal, clausal and sentential TCUs. There also exists attempts for heuristics to classify TCUs, see points below,
albeit these are not perfectly defined and it is argued about their relevance...

<div class='row' style='width: 80%;margin:auto'>
  <div class='column'>
    <strong>Type of TCU</strong>
    <ul>
      <li>Lexical TCU</li>
      <li>Phrasal TCU</li>
      <li>Clausal TCU</li>
      <li>Sentential TCU</li>
    </ul>
  </div>
  <div class='column'>
    <strong>Crieteria for TCU</strong>
    <ul>
      <li>Intonationally Complete</li>
      <li>Grammatically Complete</li>
      <li>Pragmatically Complete</li>
    </ul>
  </div>
</div>


<strong>"Grossly apparent facts", Sacks</strong>
<ul>
  <li>Speaker-change recurs, or at least occurs</li>
  <li>Overwhelmingly, <strong>one party talks at a time</strong></li>
  <li>Occurences of <strong>more than one speaker at a time</strong> are common, but <strong>brief</strong></li>
  <li> <strong>Transitions (from one turn to a next) with no gap and no overlap are common</strong>. Together with transitions characterized by slight gap or slight overlap, the make up the vast majority of transitions</li>
  <li> Distribution of turns</li>
  <ul>
    <li>Turn order is not fixed</li>
    <li>Turn size if not fixed</li>
    <li>Lenght of conversation is not specified in advance</li>
    <li>Relative distrubution of turns is not specified in advance</li>
    <li>Number of party can vary</li>
  </ul>
  <li>Talk can be continuous or discontinuous</li>
  <ul>
    <li>Continuous: Speech fluently continues at TRPs (either by a turn shift or turn hold)</li>
    <li>Discontinuous: Speech stops at a TRP and no speaker takes the turn. "More than a gap-not a gap, but lapse"
  </ul>
  <li><strong>Turn-allocation techniques are obviously used</strong>. E.g a current speaker may select the next speaker.</li>
  <li><strong>Various "turn-constructional units" are employed</strong>; e.g turns can be projectedly 'one word long', or they can be sentential in length</li>
  <li><strong>Repair mechanisms exist for dealing with turn-taking errors and violations</strong>; e.g if two parties find themselves talking at the same time, one of them will stop prematurely, thus repairing the trouble</li>
</ul>

<strong> Turn allocation compnents </strong>
<ul>
  <li>Turns allocated by <strong>current speaker's selects the next</strong> speaker</li>
  <ul>
    <li>Question</li>
    <li>greeting-greeting</li>
    <li>invitation-acceptance/decline</li>
  </ul>
  <li>Turns allocated by <strong>self-selection</strong> </li>
</ul>

<strong> Rules for turn-taking, Sacks</strong>
<ul>
  <li>For any turn, at the initial TRP of an initial TCU</li>
  <ul>
    <li>If the turn-so-far is so constructed as </li>
    <li>...involve the use of a <strong>'current speaker selects next'</strong> technique, then the party so selected has the right and is obliged to take next turn to speak; no others have such
      rights or obligations, and transfer occurs at that place</li>
    <li>...<strong>not</strong> to involve the use of a <strong>'current speaker selects next' </strong> technique, then <strong>self-selection</strong> for the next speakership may, but need not, be instituted; first starter
      acquires rights to a turn, and transfer occurs at that place.
    <li>...<strong>not</strong> to involve the use of a <strong>'current speaker selects next' </strong> technique, then current speaker may, but need not continue, unless another <strong>self-selects</strong></li>
  </ul>
  <li> It a transition dit not take plaace the rules above re-applies at the next TRP, and recursively at each next TRP until a transfer has occured.</li>
</ul>

<h2> Dialog Acts </h2>

Wikipedia: <a href="https://en.wikipedia.org/wiki/Dialog_act" target="_blank">Dialog Acts</a>

<p>
Another way to define segments in conversations are dialog acts. Dialog acts are utterances that define a function in a conversation. These are dependent on the context so that lexically equivalent utterances may be belong to
different dialog acts, have different associated functions. 
The switchboard corpus has dialog acts annotations (Jurafsky, <a href="https://web.stanford.edu/~jurafsky/ws97/" target="_blank">SWBDA</a>,  <a href="https://web.stanford.edu/~jurafsky/ws97/manual.august1.html" target="_blank">manual</a>)
</p>

<center>
  <table border="2" cellspacing="2" cellpadding="2" style='width: 60%; margin: auto'>
    <tbody>
      <tr><th>SWBD-DAMSL </th><th>SWBD</th> <th>Example </th><th>Cnt</th><th> %</th> </tr>
      <tr><td><b>Statement-non-opinion</b></td><td>sd </td><td><i> Me,  I'm in the legal department.</i> </td><td>72,824 </td><td> 36%</td> </tr>
      <tr><td><b>Acknowledge (Backchannel) </b></td><td>b</td><td> <i>Uh-huh.</i> </td><td> 37,096 </td><td> 19% </td> </tr>
      <tr><td><b>Statement-opinion</b></td><td>sv</td><td>  <i>I think it's great</i></td><td> 25,197 </td><td> 13% </td> </tr>
      <tr><td><b>Agree/Accept</b></td><td>aa</td><td> <i>That's exactly it.</i> </td><td> 10,820 </td><td> 5% </td></tr>
      <tr><td><b>Abandoned or Turn-Exit </b></td><td>% -</td><td><i>So, -</i></td><td>10,569 </td><td> 5\% </td></tr>
      <tr><td><b>Appreciation</b></td><td>ba</td><td><i>I can imagine.</i></td><td> 4,633 </td><td> 2% </td></tr>
      <tr><td><b>Yes-No-Question</b></td><td>qy</td><td><i>Do you have to have any special training?</i></td><td>4,624 </td><td> 2% </td></tr>
      <tr><td><b>Non-verbal </b></td><td> x </td><td> <i>[Laughter], [Throat_clearing]</i> </td><td>3,548 </td><td> 2% </td></tr>
      <tr><td><b>Yes answers </b></td><td>ny </td><td> <i>Yes.</i> </td><td> 2,934 </td><td> 1% </td></tr>
      <tr><td><b>Conventional-closing</b></td><td>fc</td><td><i>Well, it's been nice talking to you.</i></td><td> 2,486 </td><td>1% </td></tr>
      <tr><td><b>Uninterpretable</b></td><td> %</td><td><i>But, uh, yeah </i></td><td>2,158 </td><td> 1\% </td></tr>
      <tr><td><b>Wh-Question</b></td><td> qw</td><td><i>Well,  how old are you?</i></td><td>1,911 </td><td> 1% </td></tr>
      <tr><td><b>No answers</b></td><td>nn</td><td> <i>No.</i> </td><td> 1,340 </td><td> 1% </td></tr>
      <tr><td><b>Response Acknowledgement</b></td><td>bk</td><td><i>Oh, okay.</i></td><td>1,277</td><td> 1% </td></tr>
      <tr><td><b>Hedge</b></td><td>h</td><td> <i>I don't know if I'm making any sense or not.</i> </td><td>1,182 </td><td> 1% </td></tr>
      <tr><td><b>Declarative Yes-No-Question </b></td><td> qy^d</td><td><i>So you can afford to get a house?</i></td><td> 1,174 </td><td> 1% </td></tr>
      <tr><td><b>Other</b></td><td>o,fo,bc,by,fw</td><td> <i> Well give me a break, you know. </i></td><td>1,074 </td><td> 1% </td></tr>
      <tr><td><b>Backchannel in question form </b></td><td>bh</td><td> <i>Is that right?</i> </td><td> 1,019 </td><td> 1% </td></tr>
    </tbody> 
  </table>
</center>


<h2>Extraction of TRUs</h2>

When extracting TRPs from data we may assume that
<ul>
  <li> All shifts, realized TRPs, are TRPs. This means that at each moment the listener speeks we know that this is a TRP</li>
  <li> The unrealized TRPs are more difficult and might need Apriori definitions</li>
  <ul>
    <li> We can follow the rules for TCUs (Sacks) as a first order heuristic</li>
    <li> Grammatically complete utterances could be defined by their associated dialog act (assumes annotated data).</li>
    <li>This would include all actual TRPs at end of utterance but may omit silence segments at the end of an utterance inside an "unfinished" dialog acts</li>
    <li>A possible training metric could be to output the certainty of the current utterance as being grammatically complete. A turn may include several statements and the goal is to define the current utterance as complete
      and learn to be robust to fillers, restarts, etc. </li>
    <ul>
      <li>How many utterances are part of unfinished dialog acts?</li>
      <li>Is the noise (as compared to written langugage) sufficiently large? That is do we require actual conversational data in order to learn this or is it a "regular" NLP problem?</li>
      <li>Can this be implicitly extracted unsupervised? How?</li>
    </ul>
  </ul>

<h3>Assumptions about the data</h3>

In order to define our problem formulation we need to define what we wish to learn and if all sorts of data was available we choose the best dataset for out problem. However, the actuality is ofter reversed. We have access to
some small set of datasets and must inquire about what information is readily extractable from it. Before choosing a specifi dataset we might define the minimum requirements we wish to have.

<ol>
  <li>Audio</li>
  <ul>
    <li>Spoken dialog</li>
    <li>We wish to have continuous dialog audio because we are interested in timing and continuous prediction for spoken dialog systems</li>
  </ul>
  <li>Semantic</li>
  <ul>
    <li>We want the semantic, word, annotations available</li>
    <li>We want this information in order to define the limit of how good our models are</li>
    <li>ASR can always be used for an actual implementation which could either be as good as annotated data or worse</li>
  </ul>
  <li>Turn-taking</li>
  <ul>
    <li>Possbly train on data where TRPs are manually annotated</li>
    <li>How well may fully supervised model do?</li>
    <li>Problem: Not enough data</li>
  </ul>
</ol>

For the purposes of our work we want to constrain ourselves to data which fulfills 1, 2 in the list above. We want to infer where TRPs are based only on this.


<h4>What can we extract from such data?</h4>

<ul>
  <li> Duration of each word </li>
  <li> Duration of each IPU (requires aprori extraction definitions)</li>
  <li> Duration of each Turn (requires aprori extraction definitions)</li>
  <li> Silence after each word </li>
  <li> Silence after each IPU </li>
  <li> Silence after each Turn </li>
  <li>Mutual silence segments</li>
  <li>Overlap segments</li>
  <li>Single speaker segments</li>
</ul>


<h1>Data</h1>

<h3>Timing Features</h3>

<center>
  <div class='column'>
    <img src="/images/turntaking/semantic_base/interaction_timing_features.png" alt=""/>
    <img src="/images/turntaking/semantic_base/internal_turn_timing_features.png" alt=""/>
    <img src="/images/turntaking/semantic_base/inter_word_timing_features.png" alt=""/>
  </div>
</center>

<h3>Annotation Floor Holder </h3>
<img src="/images/turntaking/semantic_base/floor_annotated.png" alt=""/>

<h3>Annotation Floor Holder No bc</h3>
<img src="/images/turntaking/semantic_base/floor_annotated_non_bc.png" alt=""/>




<h1>Problem Formulation</h1>

<p>
The underlying goal for a turn-taking system is to correctly decide when to generate a response and when to listen. The response may fill different functions such as
acknowledgments, backchannels or grabbing the turn.
</p>


<p>
In a dialog every word uttered by a participant has a correlated silence (can be 0) before and after the duration of the word. Words at the end of turns or at turn relevant
places, TRPs, will, arguably, on average have longer correlated post-silences than their counterparts. The last word of an actual turn will have a longer associated silence
because the utterer will wait for the other participant to grab the turn in addition to the duration of the actual response. Given a TRP, where the turn could be shifted but
wasn't, the turn yielder will, on average, wait for a response for some duration before resolving the resulted silence.
</p>

<p>
Let each spoken word, \(x_i\), in a dialog corpus have an associated post-silence, \(y_i\) and learn to classify \(y_i = F(x_{i\geq})\). The values \(y_i \in Y\) can either be
classified as discretized bins or as continuous values predicted through regression.
</p>

<h1>Data</h1>


The data corpus we focus on is Switchboard, a popular dialog dataset consiting of phone conversations. The corpus have annotated words and their assoiciated time of utterence
refered to here as timed words.


Lets extract all the timed words from the corpus along with their associated post-silence and their duration for good measure. Below the top 50 words ordered by frequency, post-silence and duration are
shown.


<h3>Annotation</h3>


all words untokenized
<div class='row'>
  <div class='column'>
    <img src="/images/turntaking/semantic_base/Silences_swb.png" alt="" style='width: 90%'/>
  </div>
  <div class='column'>
    <img src="/images/turntaking/semantic_base/Silences_maptask.png" alt="" style='width: 90%'/>
  </div>
</div>



<h3>Tokenized</h3>

<img src="/images/turntaking/semantic_base/tok_top_freq.png" alt=""/>
<img src="/images/turntaking/semantic_base/tok_top_sil.png" alt=""/>
<img src="/images/turntaking/semantic_base/tok_top_dur.png" alt=""/>
