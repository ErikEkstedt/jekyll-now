---
layout: post
title: Information
---


<!--excerpt-->

## KL Divergence


## Mutual Information

![mutual_information.png](/assets/images/Information/mutual_information.png)

"How much uncertainty in one variable is reduced by knowing another variable" [The Information Bottleneck Theory, Naftali Tishby](https://www.youtube.com/watch?v=EQTtBRM0sIs)


Let the input to a neural network be X, a layer Y and the output Z. Then the mutual
information between the input and the output will follow:

![data_processing_inequality_DPI_and_invariance.png](/assets/images/Information/data_processing_inequality_DPI_and_invariance.png)
