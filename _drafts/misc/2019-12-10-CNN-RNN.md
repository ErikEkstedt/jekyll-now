---
layout: post
author: Erik
---

# CNN-RNN


How to use a CNN over arbitrary long sequences as encoder to an RNN? How to make sure that no
information flows "backwards" in time the deeper in the network it gets. How to construct a strong
1Dconv encoder for spectrogram/MFCC?


<img class='centerImg' src="/images/turntaking/tt_cnn/CNN_layers._Batchnorm:_False,_Skip_connection:_False.png" alt="" width="40%"/>


<!--more-->


#### Inspiration:
* [WaveNet, van der Oord, 2016](https://arxiv.org/abs/1609.03499)
    - [vincentherrmann/pytorch-wavenet)](https://github.com/vincentherrmann/pytorch-wavenet)


# Sound

* Audio is sampled by pressure changes in the air. 
* These changes are measured by a device to produce discrete values at certain time intervals
* 1D sequence over time
* 1D -> 2D, $$R^1 \to R^2$$, fourier transformation.
  - A fuzzier perspective (to a degree) over time
  - Image space where left to right carry different meaning than right to left; information is not symmetric about the frequency-axis
* Samples - Spectrogram
    - Frequenciy Intensity over Time
    - Human hearing $$20-20KHz$$
    - 16Khz for speech intuitevely seems plenty to have conversations
    - 8KHz is arguably enugh but the quality degregation is at least annoying. Too much noice is
        difficult to encode
* more complex the content of the information in the audio the more other capabilities more than audio quality is needed. For simpler speech activities I would argue 4Khz is mostly enough even though here you might be bothered (and lose concentration) by the audio quality.



On Deepminds [blog](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) they show this gif zooming in on an audio waveform.
<img width='200' src="https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig1-Anim-160908-r01.gif" alt="" align='middle'> 

A sequence of intensity over time.  A [Fourier Transform](https://en.wikipedia.org/wiki/Fourier_transform)
decomposes the signal in to a frequency domain representation, precicely a sum over frequencies and their magnitude. A linear set of
frequencies played with different magnitudes.

<iframe width="200" src="https://www.youtube.com/embed/spUNpyF58BY?ecver=1" frameborder="0" allow="autoplay; encrypted-media" align='middle' allowfullscreen> All possible sounds span sound space  </iframe> 
<iframe width="200" src="https://s3.envato.com/h264-video-previews/87821136-dccf-4270-a5a3-57f6cca7fde8/20404750.mp4" align='middle' frameborder="90" allow="autoplay; encrypted-media" allowfullscreen></iframe>


A Mel-Spectrogram over a phrase. Constructed by phonetic building blocks "quite easily"
distinguishable under good circumstances.


<div class="column">
  <img src="/images/turntaking/tt_analysis/nobody_understands_quantum_mechanics_spec.png" alt="ALL" style='width: 80%'>
</div>


## Implementation


A simple cnn stack is constructed with 5 layers or cnn-modules where each module contains a Conv1d,
BatchNorm1d(optional) and an activation (relu). In the forward pass of the layer stack skip
connections may be used. In order to use such convolutional layers as an encoder to an RNN for a time
sequence of arbitrary length it is vital that the information in the input $$ x_t $$ does not flow
"backwards" in time further into/up the network layers.

To ensure this we make sure to pad our input (to the left), to any convolutional layer, with $$ Pad
= kernel\_size-1 $$ zeros. Instead of having the convolutional kernel centered on the current time
step $$ x_t $$ with information being encoded on both sides we move everything to the right such
that the current time frame is at the very end of our kernel span. The code for the cnn-module is
shown below.


<button onclick="cnnModule()">CNN Module</button>


<div id="cnnModule">
~~~ python
def cnn_module(
    in_channels,
    out_channels,
    kernel_size,
    stride=1,
    pad=0,
    bias=True,
    act="relu",
    batchnorm=False,
):
    if batchnorm:
        return nn.Sequential(
            nn.ConstantPad1d((pad, 0), 0),  # left side padding
            nn.Conv1d(
                in_channels,
                out_channels,
                stride=stride,
                kernel_size=kernel_size,
                bias=bias,
            ),
            nn.BatchNorm1d(out_channels),
            get_nn_activation(act)(),
        )
    else:
        return nn.Sequential(
            nn.ConstantPad1d((pad, 0), 0),  # left side padding
            nn.Conv1d(
                in_channels,
                out_channels,
                stride=stride,
                kernel_size=kernel_size,
                bias=bias,
            ),
            get_nn_activation(act)(),
        )

~~~
</div>

A stack is the collections of these modules (with optional skip-connections and dropout) and the
code i shown below.

<button onclick="cnnStack()">CNN Stack</button>

<div id="cnnStack">
~~~python
class CnnStack(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg

        self.skip = cfg["skip_connections"]
        self.convs = nn.ModuleList()
        # self.filer = nn.ModuleList()
        # self.gate = nn.ModuleList()

        self.dropout = nn.Dropout(p=cfg["dropout"])

        for i in range(cfg["layers"]):
            if i == 0:
                self.convs.append(
                    cnn_module(
                        in_channels=cfg["in_channels"],
                        out_channels=cfg["hidden_channels"],
                        kernel_size=cfg["kernel_size"],
                        stride=1,
                        pad=cfg["kernel_size"] - 1,
                        bias=cfg["bias"],
                        act=cfg["activation"],
                        batchnorm=cfg["batchnorm"],
                    )
                )
            else:
                self.convs.append(
                    cnn_module(
                        in_channels=cfg["hidden_channels"],
                        out_channels=cfg["hidden_channels"],
                        kernel_size=cfg["kernel_size"],
                        stride=1,
                        pad=cfg["kernel_size"] - 1,
                        bias=cfg["bias"],
                        act=cfg["activation"],
                        batchnorm=cfg["batchnorm"],
                    )
                )

    def fill_(self, value=1):
        for name, param in self.named_parameters():
            if "weight" in name:
                param.data.fill_(value)

    def forward(self, x):

        for i, layer in enumerate(self.convs):
            z = layer(x)

            # Residual connections
            if self.skip and (i > 0 and z.shape[-1] == x.shape[-1]):
                z = z + x

            z = self.dropout(z)

            x = z

        return z
~~~

</div>

## Sanity Check

<div style='padding: 0 40px'>
  
Lets test this on a simple input with zeros everywhere except at one frame. We initialize our
convolutional stacks with weights of all ones with no bias. This will propogate all the information
(albeit scaled) into the next layer. Below is an image where the input layer is at the bottom and 4
5-layer-cnn-stacks with varying kernel sizes are displayed above. What is shown in the images are
the output from these different stacks. From the bottom up we have Input, 1, 3, 5, 7 kernel sizes
respectively.
</div>


<div class='row' width='80%'>
  <img class='centerImg' width="40%" src="/images/turntaking/tt_cnn/CNNStacks.png" alt=""/>
  <img class='centerImg' width="40%" src="/images/turntaking/tt_cnn/CNNEncoder.png" alt=""/>
</div>
Left: CNN stacks with different kernel sizes but the same number of layers. Right: CNN Encoder. Simply a wrapper around many CNN Stacks and concatenates the output.

<div style='padding: 0 40px'>
    The thing we want to make sure does not happen is that the information, the
    activation in this case, does NOT flow up towards the left. 
</div>

This would indicate that input information in $$ x_t $$ is used for the output representation $$ z_{<t} $$.


## Adding Features

In order to make sure that the same is true when adding additional settings we test the stacks with
BatchNorm1d and skip-connections.

<div class='row'>
  <div class='column'>
    <b>BatchNorm</b>
    <img class='centerImg' width="100%" src="/images/turntaking/tt_cnn/CNN_layers._Batchnorm:_True,_Skip_connection:_False.png" alt=""/>
  </div>
  <div class='column'>
  <b>Skip Connections</b>
  <img class='centerImg' width="100%" src="/images/turntaking/tt_cnn/CNN_layers._Batchnorm:_False,_Skip_connection:_True.png" alt=""/>
  </div>
  <div class='column'>
    <b>SC + BN</b>
    <img class='centerImg' width="100%" src="/images/turntaking/tt_cnn/CNN_layers._Batchnorm:_True,_Skip_connection:_True.png" alt=""/>
  </div>
</div>

## Effect of Stride


## Effect of dilation

## Conversation & Prosody

<button onclick="nonSense()"> The Nonsensicality of Prosody with seemingly random spoken information.  </button>

[Wikipedia](https://en.wikipedia.org/wiki/Music_for_a_French_Elevator_and_Other_Short_Format_Oddities_by_the_Books)
Wikipedia contributors. "Music_for_a_French_Elevator_and_Other_Short_Format_Oddities_by_the_Books" Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 20 Dec. 2019. Web. 20 Dec. 2019.

<div id="nonSense">


  <li> The Books makes cool art.  </li>

  <blockquote>
    "It is a compendium on mini CD of four pieces created for the "1%" art and sound installation in the Ministry of Culture in Paris, France in 2004." 
  </blockquote>


  <blockquote>
    I laughed really hard one night at around 22.30 in front of my computer
    "Can you shut the door!"
  </blockquote>

  <iframe src="https://open.spotify.com/embed/track/6q6EXUhoujkQCkcAwe1jEK" width="300" height="380" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

  Music For a French and Other short format oddities, The Books, 2004 [pitchfork article](https://pitchfork.com/reviews/albums/857-music-for-a-french-elevator/)
  We hear some conjunctions "and", "or", some other "not totally random" words but mostly it is
  numbers. (min - max)

</div>

