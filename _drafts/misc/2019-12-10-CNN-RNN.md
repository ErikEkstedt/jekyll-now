---
layout: post
author: Erik
---

# CNN-RNN


How to use a CNN over arbitrary long sequences as encoder to an RNN? How to make sure that no
information flows "backwards" in time the deeper in the network it gets. How to construct a strong
1Dconv encoder for spectrogram/MFCC?


<img class='centerImg' src="/images/turntaking/tt_cnn/CNN_layers._Batchnorm:_False,_Skip_connection:_False.png" alt="" width="40%"/>


<!--more-->


#### Inspiration:
* [WaveNet, van der Oord, 2016](https://arxiv.org/abs/1609.03499)
    - [vincentherrmann/pytorch-wavenet)](https://github.com/vincentherrmann/pytorch-wavenet)


## Implementation

A simple cnn stack is constructed with 5 layers or cnn-modules where each module contains a Conv1d,
BatchNorm1d(optional) and an activation (relu). In the forward pass of the layer stack skip
connections may be used. In order to use such convolutional layers as an encoder to an RNN for a time
sequence of arbitrary length it is vital that the information in the input $$ x_t $$ does not flow
"backwards" in time further into/up the network layers.

To ensure this we make sure to pad our input (to the left), to any convolutional layer, with $$ Pad
= kernel\_size-1 $$ zeros. Instead of having the convolutional kernel centered on the current time
step $$ x_t $$ with information being encoded on both sides we move everything to the right such
that the current time frame is at the very end of our kernel span. The code for the cnn-module is
shown below.

~~~ python
def cnn_module(
    in_channels,
    out_channels,
    kernel_size,
    stride=1,
    pad=0,
    bias=True,
    act="relu",
    batchnorm=False,
):
    if batchnorm:
        return nn.Sequential(
            nn.ConstantPad1d((pad, 0), 0),  # left side padding
            nn.Conv1d(
                in_channels,
                out_channels,
                stride=stride,
                kernel_size=kernel_size,
                bias=bias,
            ),
            nn.BatchNorm1d(out_channels),
            get_nn_activation(act)(),
        )
    else:
        return nn.Sequential(
            nn.ConstantPad1d((pad, 0), 0),  # left side padding
            nn.Conv1d(
                in_channels,
                out_channels,
                stride=stride,
                kernel_size=kernel_size,
                bias=bias,
            ),
            get_nn_activation(act)(),
        )

~~~

A stack is the collections of these modules (with optional skip-connections and dropout) and the
code i shown below.

~~~python
class CnnStack(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg

        self.skip = cfg["skip_connections"]
        self.convs = nn.ModuleList()
        # self.filer = nn.ModuleList()
        # self.gate = nn.ModuleList()

        self.dropout = nn.Dropout(p=cfg["dropout"])

        for i in range(cfg["layers"]):
            if i == 0:
                self.convs.append(
                    cnn_module(
                        in_channels=cfg["in_channels"],
                        out_channels=cfg["hidden_channels"],
                        kernel_size=cfg["kernel_size"],
                        stride=1,
                        pad=cfg["kernel_size"] - 1,
                        bias=cfg["bias"],
                        act=cfg["activation"],
                        batchnorm=cfg["batchnorm"],
                    )
                )
            else:
                self.convs.append(
                    cnn_module(
                        in_channels=cfg["hidden_channels"],
                        out_channels=cfg["hidden_channels"],
                        kernel_size=cfg["kernel_size"],
                        stride=1,
                        pad=cfg["kernel_size"] - 1,
                        bias=cfg["bias"],
                        act=cfg["activation"],
                        batchnorm=cfg["batchnorm"],
                    )
                )

    def fill_(self, value=1):
        for name, param in self.named_parameters():
            if "weight" in name:
                param.data.fill_(value)

    def forward(self, x):

        for i, layer in enumerate(self.convs):
            z = layer(x)

            # Residual connections
            if self.skip and (i > 0 and z.shape[-1] == x.shape[-1]):
                z = z + x

            z = self.dropout(z)

            x = z

        return z
~~~


## Evaluation

Lets test this on a simple input with zeros everywhere except at one frame. We initialize our
convolutional stacks with weights of all ones with no bias. This will propogate all the information
(albeit scaled) into the next layer. Below is an image where the input layer is at the bottom and 4
5-layer-cnn-stacks with varying kernel sizes are displayed above. What is shown in the images are
the output from these different stacks. From the bottom up we have Input, 1, 3, 5, 7 kernel sizes
respectively.


<img class='centerImg' width="70%" src="/images/turntaking/tt_cnn/CNN_layers._Batchnorm:_False,_Skip_connection:_False.png" alt=""/>

The thing we want to make sure does not happen is that the information, the
activation in this case, does NOT flow up towards the left. This would indicate
that input information in $$ x_t $$ is used for the output representation $$ z_{<t} $$.


## Adding Features

In order to make sure that the same is true when adding additional settings we test the stacks with
BatchNorm1d and skip-connections.

<div class='row'>
  
  <div class='column'>
    <b>BatchNorm</b>
    <img class='centerImg' width="100%" src="/images/turntaking/tt_cnn/CNN_layers._Batchnorm:_True,_Skip_connection:_False.png" alt=""/>
  </div>
  <div class='column'>
  <b>Skip Connections</b>
  <img class='centerImg' width="100%" src="/images/turntaking/tt_cnn/CNN_layers._Batchnorm:_False,_Skip_connection:_True.png" alt=""/>
  </div>
  <div class='column'>
    <b>Skip Connections and Batch Norm</b>
    <img class='centerImg' width="100%" src="/images/turntaking/tt_cnn/CNN_layers._Batchnorm:_True,_Skip_connection:_True.png" alt=""/>
  </div>
</div>


## Effect of Stride

## Effect of dilatetion
