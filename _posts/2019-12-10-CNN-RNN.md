---
layout: post
author: Erik
---

# CNN-RNN


How to use a CNN over arbitrary long sequences as encoder to an RNN? How to make sure that no
information flows "backwards" in time the deeper in the network it gets.


<img class='centerImg' src="/images/turntaking/tt_cnn/CNN_layers._Batchnorm:_False,_Skip_connection:_False.png" alt="" width="40%"/>


<!--more-->


CNN-1D encoders with 5 layers and different kernel sizes. Some notion of the
effects of batchnorm and skip connections.

A simple cnn stack is constructed with 5 layers where each layer contains a conv-module: Conv1d, (optional) BatchNorm1d and an activation (relu). In the forward pass of the layer stack skip connections may be used. In order to use such convolutional layers an encoder to an RNN for a time sequence of arbitrary length it is vital that the information in the input $$ x_t $$ does not flow "backwards" in time further into/up the network layers.

To ensure this we make sure to pad our input (to the left), to any convolutional layer, with $$ Pad = kernel\_size-1 $$ zeros. Instead of having the convolutional kernel centered on the current time step $$ x_t $$ with information being encoded on both sides we move everything to the right such that the current time frame is at the very end of our kernel span.

Lets test this on a simpe input with zeros everywhere except at one frame. We initialize our convolutional stacks with weights of all ones with no bias. This will propogate all the information into the next layer. Below is an image where the input layer is at the bottom and 4 5-layer-cnn-stacks with varying kernel sizes are displayed above. What is shown in the images are the output from these different stacks. From the bottom up we have Input, 1, 3, 5, 7 kernel sizes respectively.


<img class='centerImg' width="70%" src="/images/turntaking/tt_cnn/CNN_layers._Batchnorm:_False,_Skip_connection:_False.png" alt=""/>

The thing we want to make sure does not happen is that the information, the
activation in this case, does NOT flow up towards the left. This would indicate
that input information in $$ x_t $$ is used for the output representation $$ z_{<t} $$.


## Adding Features

In order to make sure that the same is true when adding additional settings we test the stacks with
BatchNorm1d and skip-connections.

### BatchNorm


<img class='centerImg' width="70%" src="/images/turntaking/tt_cnn/CNN_layers._Batchnorm:_True,_Skip_connection:_False.png" alt=""/>

### Skip Connections

<img class='centerImg' width="70%" src="/images/turntaking/tt_cnn/CNN_layers._Batchnorm:_False,_Skip_connection:_True.png" alt=""/>

### Skip Connections and Batch Norm

<img class='centerImg' width="70%" src="/images/turntaking/tt_cnn/CNN_layers._Batchnorm:_True,_Skip_connection:_True.png" alt=""/>


